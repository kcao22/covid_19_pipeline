{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import boto3\n",
    "import configparser\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "import time\n",
    "\n",
    "from helper_functions import create_session\n",
    "from io import StringIO\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate client connections\n",
    "session = create_session()\n",
    "glue = session.client('glue')\n",
    "athena = session.client('athena')\n",
    "s3 = session.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of tables in Glue database\n",
    "database_name = 'aws-covid-project'\n",
    "bucket_name = 'kc-covid-project'\n",
    "table_tuple = tuple([table['Name'] for table in glue.search_tables()['TableList']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n"
     ]
    }
   ],
   "source": [
    "# Query schemas for tables using Athena, save to S3\n",
    "try:\n",
    "    query = f'SELECT * FROM INFORMATION_SCHEMA.columns WHERE table_name IN {(table_tuple)}'\n",
    "    res = athena.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={'Database':database_name},\n",
    "        ResultConfiguration={\n",
    "            'OutputLocation':'s3://kc-covid-project/athena_outputs/',\n",
    "            'EncryptionConfiguration':{'EncryptionOption':'SSE_S3'}\n",
    "                            },\n",
    "    )\n",
    "    print(f'Status Code: {res[\"ResponseMetadata\"][\"HTTPStatusCode\"]}')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminal command create empty csv file to save schema file to\n",
    "! type nul > athena_outputs/table_schemas.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download schema file from s3 bucket, load into Pandas\n",
    "try:\n",
    "    schema_csv = s3.list_objects(\n",
    "        Bucket=bucket_name,\n",
    "        Prefix='athena_outputs'\n",
    "    )['Contents'][0]['Key']\n",
    "    # print(schema_csv)\n",
    "    res = s3.download_file(\n",
    "            Bucket=bucket_name,\n",
    "            Key=schema_csv,\n",
    "            Filename='athena_outputs/table_schemas.csv'\n",
    "        )\n",
    "    print(f'Status Code: {res[\"ResponseMetadata\"][\"HTTPStatusCode\"]}')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table_catalog</th>\n",
       "      <th>table_schema</th>\n",
       "      <th>table_name</th>\n",
       "      <th>column_name</th>\n",
       "      <th>ordinal_position</th>\n",
       "      <th>column_default</th>\n",
       "      <th>is_nullable</th>\n",
       "      <th>data_type</th>\n",
       "      <th>comment</th>\n",
       "      <th>extra_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>awsdatacatalog</td>\n",
       "      <td>aws-covid-project</td>\n",
       "      <td>enigma_nyt_usa_states</td>\n",
       "      <td>date</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>YES</td>\n",
       "      <td>varchar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>awsdatacatalog</td>\n",
       "      <td>aws-covid-project</td>\n",
       "      <td>enigma_nyt_usa_states</td>\n",
       "      <td>state</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>YES</td>\n",
       "      <td>varchar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>awsdatacatalog</td>\n",
       "      <td>aws-covid-project</td>\n",
       "      <td>enigma_nyt_usa_states</td>\n",
       "      <td>fips</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>YES</td>\n",
       "      <td>bigint</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>awsdatacatalog</td>\n",
       "      <td>aws-covid-project</td>\n",
       "      <td>enigma_nyt_usa_states</td>\n",
       "      <td>cases</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>YES</td>\n",
       "      <td>bigint</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>awsdatacatalog</td>\n",
       "      <td>aws-covid-project</td>\n",
       "      <td>enigma_nyt_usa_states</td>\n",
       "      <td>deaths</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>YES</td>\n",
       "      <td>bigint</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>awsdatacatalog</td>\n",
       "      <td>aws-covid-project</td>\n",
       "      <td>rearc_usa_latest_total</td>\n",
       "      <td>hospitalized</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>YES</td>\n",
       "      <td>bigint</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>awsdatacatalog</td>\n",
       "      <td>aws-covid-project</td>\n",
       "      <td>rearc_usa_latest_total</td>\n",
       "      <td>total</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>YES</td>\n",
       "      <td>bigint</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>awsdatacatalog</td>\n",
       "      <td>aws-covid-project</td>\n",
       "      <td>rearc_usa_latest_total</td>\n",
       "      <td>totaltestresults</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>YES</td>\n",
       "      <td>bigint</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>awsdatacatalog</td>\n",
       "      <td>aws-covid-project</td>\n",
       "      <td>rearc_usa_latest_total</td>\n",
       "      <td>posneg</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>YES</td>\n",
       "      <td>bigint</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>awsdatacatalog</td>\n",
       "      <td>aws-covid-project</td>\n",
       "      <td>rearc_usa_latest_total</td>\n",
       "      <td>notes</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>YES</td>\n",
       "      <td>varchar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      table_catalog       table_schema              table_name  \\\n",
       "0    awsdatacatalog  aws-covid-project   enigma_nyt_usa_states   \n",
       "1    awsdatacatalog  aws-covid-project   enigma_nyt_usa_states   \n",
       "2    awsdatacatalog  aws-covid-project   enigma_nyt_usa_states   \n",
       "3    awsdatacatalog  aws-covid-project   enigma_nyt_usa_states   \n",
       "4    awsdatacatalog  aws-covid-project   enigma_nyt_usa_states   \n",
       "..              ...                ...                     ...   \n",
       "153  awsdatacatalog  aws-covid-project  rearc_usa_latest_total   \n",
       "154  awsdatacatalog  aws-covid-project  rearc_usa_latest_total   \n",
       "155  awsdatacatalog  aws-covid-project  rearc_usa_latest_total   \n",
       "156  awsdatacatalog  aws-covid-project  rearc_usa_latest_total   \n",
       "157  awsdatacatalog  aws-covid-project  rearc_usa_latest_total   \n",
       "\n",
       "          column_name  ordinal_position  column_default is_nullable data_type  \\\n",
       "0                date                 1             NaN         YES   varchar   \n",
       "1               state                 2             NaN         YES   varchar   \n",
       "2                fips                 3             NaN         YES    bigint   \n",
       "3               cases                 4             NaN         YES    bigint   \n",
       "4              deaths                 5             NaN         YES    bigint   \n",
       "..                ...               ...             ...         ...       ...   \n",
       "153      hospitalized                14             NaN         YES    bigint   \n",
       "154             total                15             NaN         YES    bigint   \n",
       "155  totaltestresults                16             NaN         YES    bigint   \n",
       "156            posneg                17             NaN         YES    bigint   \n",
       "157             notes                18             NaN         YES   varchar   \n",
       "\n",
       "     comment  extra_info  \n",
       "0        NaN         NaN  \n",
       "1        NaN         NaN  \n",
       "2        NaN         NaN  \n",
       "3        NaN         NaN  \n",
       "4        NaN         NaN  \n",
       "..       ...         ...  \n",
       "153      NaN         NaN  \n",
       "154      NaN         NaN  \n",
       "155      NaN         NaN  \n",
       "156      NaN         NaN  \n",
       "157      NaN         NaN  \n",
       "\n",
       "[158 rows x 10 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After loading schema file into Pandas DataFrame, use DataFrame below to design data model\n",
    "# Alternatively, generate DDL from tables in Glue database\n",
    "table_schemas = pd.read_csv('athena_outputs/table_schemas.csv', index_col=None)\n",
    "table_schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for athena query outputs, look for file to download\n",
    "# s3_resource = session.resource('s3')\n",
    "# project_bucket = s3_resource.Bucket(bucket_name)\n",
    "# objs = project_bucket.objects.filter(Prefix='athena_outputs')\n",
    "# for obj in objs:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download athena output query from bucket\n",
    "s3.download_file(\n",
    "            Bucket=bucket_name,\n",
    "            Key='static-state-codes',\n",
    "            Filename='athena_outputs/state_codes.csv'\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relational Modeling\n",
    "\n",
    "### Fact Table\n",
    "\n",
    " - Contains keys and COVID-19 related information.\n",
    "\n",
    "    - Fips (primary key for most of the datasets)\n",
    "    - State & region keys, hospitals, dates (dimension tables)\n",
    "    - Statistics from datasetes regarding COVID-19 (positive cases, negative cases, deaths, etc.)\n",
    "\n",
    "- Fields for data modeling are from Athena INFORMATION_SCHEMA.columns based on shared keys & STAR schema data modeling concepts.\n",
    "\n",
    "    - Fact Table\n",
    "        - fips\n",
    "        - date\n",
    "        - states\n",
    "        - regions\n",
    "        - confirmed\n",
    "        - deaths\n",
    "        - recovered\n",
    "        - active\n",
    "        - positive\n",
    "        - negative\n",
    "        - hospitalizedcurrently\n",
    "        - hospitalized\n",
    "        - hospitalizeddischarged\n",
    "\n",
    "### Dimension tables\n",
    " - Contains dimensional keys and dimension-specific information\n",
    " \n",
    "    - Region Dimension Table\n",
    "        - fips\n",
    "        - region\n",
    "        - state\n",
    "        - county\n",
    "        - state_abb\n",
    "        - lat\n",
    "        - long\n",
    "\n",
    "    - Hospital Dimension Table\n",
    "        - fips\n",
    "        - state\n",
    "        - hos_lat\n",
    "        - hos_long\n",
    "        - hq_address\n",
    "        - hospital_type\n",
    "        - hospital_name\n",
    "        - hq_city\n",
    "        - hq_state\n",
    "\n",
    "    - Date Dimension Table\n",
    "        - fips\n",
    "        - date\n",
    "        - month\n",
    "        - year\n",
    "        - is_weekend\n",
    "\n",
    "### Process\n",
    " 1. Build relational schema for above\n",
    " 2. Connect to Athena and query table data\n",
    " 3. Write ETL jobs in Python\n",
    " 4. Build table schema to Redshift WH\n",
    " 5. Copy data to Redshift WH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Database Variables\n",
    "parser = configparser.ConfigParser()\n",
    "parser.read_file(open('relational_modeling.config'))\n",
    "AWS_REGION = parser.get('DB', 'AWS_REGION')\n",
    "SCHEMA_NAME = parser.get('DB', 'SCHEMA_NAME')\n",
    "S3_STAGING_DIR = parser.get('DB', 'S3_STAGING_DIR')\n",
    "S3_BUCKET_NAME = parser.get('DB', 'S3_BUCKET_NAME')\n",
    "S3_OUTPUT_DIRECTORY = parser.get('DB', 'S3_OUTPUT_DIRECTORY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master('local') \\\n",
    "    .appName('awscovidproject') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying table data and storing with Pandas; verify process scripts work as intended\n",
    "\n",
    "# Create function\n",
    "def dl_load_query(athena_client, s3_client, spark, dataset):\n",
    "    '''\n",
    "    ARGUMENTS:\n",
    "        athena_client: AWS boto3 client object for conencting to AWS Athena service.\n",
    "        s3_client: AWS boto3 client object for connecting to s3\n",
    "        dataset: Name of dataset in Covid Database to query data from\n",
    "        spark: SparkSession for PySpark\n",
    "    RETURNS:\n",
    "        Pandas DataFrame of query request to AWS Athena\n",
    "    '''\n",
    "    try:\n",
    "        # Get query response dictionary\n",
    "        start_res = athena_client.start_query_execution(\n",
    "            QueryString=f'SELECT * FROM {dataset}',\n",
    "            QueryExecutionContext={\n",
    "                'Database':SCHEMA_NAME,\n",
    "            },\n",
    "            ResultConfiguration={\n",
    "                'OutputLocation': S3_STAGING_DIR,\n",
    "                'EncryptionConfiguration':{'EncryptionOption':'SSE_S3'}\n",
    "            }\n",
    "        )\n",
    "        # If status request == 200, try and get query results unless query is still running\n",
    "        while start_res[\"ResponseMetadata\"][\"HTTPStatusCode\"] == 200:\n",
    "            try:\n",
    "                get_res = athena_client.get_query_results(\n",
    "                    QueryExecutionId=start_res['QueryExecutionId']\n",
    "                )\n",
    "                break\n",
    "            # If query still running, wait 0.01s\n",
    "            except Exception as e:\n",
    "                if 'not yet finished' in str(e):\n",
    "                    time.sleep(0.01)\n",
    "                else:\n",
    "                    raise e\n",
    "        file_save_path = f'athena_outputs/{dataset}.csv'\n",
    "        dl_res = s3_client.download_file(\n",
    "            Bucket=S3_BUCKET_NAME,\n",
    "            Key=f'{S3_OUTPUT_DIRECTORY}/{start_res[\"QueryExecutionId\"]}.csv',\n",
    "            Filename=file_save_path\n",
    "        )\n",
    "        return spark.read.csv(file_save_path, header=True)\n",
    "    except Exception as e:\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all Athena queried data as Pandas DataFrames. Map with dictionary to avoid redundant function call\n",
    "datasets = [table['Name'] for table in glue.search_tables()['TableList']]\n",
    "dfs = {\n",
    "    key: None for key in datasets\n",
    "}\n",
    "for table in datasets:\n",
    "    if table not in os.listdir('athena_outputs'):\n",
    "        dfs[table] = dl_load_query(athena, s3, table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['static_state_codes'] = spark.read.csv('athena_outputs/static_state_codes.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addressing static_state_codes DataFrame. Because all colunmns are type string, reading headers\n",
    "headers = dfs['static_state_codes'].first()[:]\n",
    "dfs['static_state_codes'] = dfs['static_state_codes'].filter((F.col('col0') != 'State') & (F.col('col1') != 'Abbreviation'))\n",
    "for col, new_col in zip(dfs['static_state_codes'].columns, headers):\n",
    "    dfs['static_state_codes'] = dfs['static_state_codes'].withColumnRenamed(col, new_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|               State|Abbreviation|\n",
      "+--------------------+------------+\n",
      "|             Alabama|          AL|\n",
      "|              Alaska|          AK|\n",
      "|             Arizona|          AZ|\n",
      "|            Arkansas|          AR|\n",
      "|          California|          CA|\n",
      "|            Colorado|          CO|\n",
      "|         Connecticut|          CT|\n",
      "|            Delaware|          DE|\n",
      "|District of Columbia|          DC|\n",
      "|             Florida|          FL|\n",
      "|             Georgia|          GA|\n",
      "|              Hawaii|          HI|\n",
      "|               Idaho|          ID|\n",
      "|            Illinois|          IL|\n",
      "|             Indiana|          IN|\n",
      "|                Iowa|          IA|\n",
      "|              Kansas|          KS|\n",
      "|            Kentucky|          KY|\n",
      "|           Louisiana|          LA|\n",
      "|               Maine|          ME|\n",
      "+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs['static_state_codes'].show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fact Table Fields, Data Lineage, and Join Plan\n",
    "    - Fact Table\n",
    "        - fips -> enigma_jhu, rearc_states_daily_test\n",
    "        - date -> rearc_states_daily_test\n",
    "        - states -> enigma_jhu\n",
    "        - regions -> enigma_jhu\n",
    "        - confirmed -> enigma_jhu\n",
    "        - deaths -> enigma_jhu\n",
    "        - recovered -> enigma_jhu\n",
    "        - active -> enigma_jhu\n",
    "        - positive -> rearc_states_daily_test\n",
    "        - negative -> rearc_states_daily_test\n",
    "        - hospitalizedcurrently -> rearc_states_daily_test\n",
    "        - hospitalized -> rearc_states_daily_test\n",
    "        - hospitalizeddischarged -> rearc_states_daily_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+--------------+---------+------+---------+------+\n",
      "|fips|province_state|country_region|confirmed|deaths|recovered|active|\n",
      "+----+--------------+--------------+---------+------+---------+------+\n",
      "|null|         Anhui|         China|        1|  null|     null|  null|\n",
      "|null|       Beijing|         China|       14|  null|     null|  null|\n",
      "|null|     Chongqing|         China|        6|  null|     null|  null|\n",
      "|null|        Fujian|         China|        1|  null|     null|  null|\n",
      "|null|         Gansu|         China|     null|  null|     null|  null|\n",
      "|null|     Guangdong|         China|       26|  null|     null|  null|\n",
      "|null|       Guangxi|         China|        2|  null|     null|  null|\n",
      "|null|       Guizhou|         China|        1|  null|     null|  null|\n",
      "|null|           Hai|         China|        4|  null|     null|  null|\n",
      "|null|         Hebei|         China|        1|  null|     null|  null|\n",
      "|null|  Heilongjiang|         China|     null|  null|     null|  null|\n",
      "|null|            He|         China|        5|  null|     null|  null|\n",
      "|null|     Hong Kong|     Hong Kong|     null|  null|     null|  null|\n",
      "|null|         Hubei|         China|      444|    17|       28|  null|\n",
      "|null|            Hu|         China|        4|  null|     null|  null|\n",
      "|null|Inner Mongolia|         China|     null|  null|     null|  null|\n",
      "|null|       Jiangsu|         China|        1|  null|     null|  null|\n",
      "|null|       Jiangxi|         China|        2|  null|     null|  null|\n",
      "|null|         Jilin|         China|     null|  null|     null|  null|\n",
      "|null|      Liaoning|         China|        2|  null|     null|  null|\n",
      "+----+--------------+--------------+---------+------+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs['enigma_jhu'].select('fips', 'province_state', 'country_region', 'confirmed', 'deaths', 'recovered', 'active').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+--------+--------+---------------------+------------+----------------------+\n",
      "|fips|    date|positive|negative|hospitalizedcurrently|hospitalized|hospitalizeddischarged|\n",
      "+----+--------+--------+--------+---------------------+------------+----------------------+\n",
      "|  49|20210220|  366034| 1507875|                  260|       14421|                  null|\n",
      "|  51|20210220|  561812|    null|                 1594|       23436|                 45667|\n",
      "|  78|20210220|    2575|   43564|                 null|        null|                  null|\n",
      "|  50|20210220|   14359|  309335|                   39|        null|                  null|\n",
      "|  53|20210220|  332904|    null|                  608|       18969|                  null|\n",
      "|  55|20210220|  611789| 2588501|                  370|       25716|                  null|\n",
      "|  54|20210220|  129364|    null|                  292|        null|                  null|\n",
      "|  56|20210220|   53683|  178648|                   31|        1367|                  null|\n",
      "|   2|20210219|   55198|    null|                   34|        1243|                  null|\n",
      "|   1|20210219|  485212| 1867861|                  951|       44767|                  null|\n",
      "|   5|20210219|  314713| 2348207|                  630|       14500|                  null|\n",
      "|  60|20210219|       0|    2140|                 null|        null|                  null|\n",
      "|   4|20210219|  804116| 2918905|                 1738|       56732|                113697|\n",
      "|   6|20210219| 3428518|    null|                 8156|        null|                  null|\n",
      "|   8|20210219|  418695| 2126610|                  451|       23101|                 22202|\n",
      "|   9|20210219|  273101|    null|                  535|       12257|                  null|\n",
      "|  11|20210219|   39461|    null|                  209|        null|                  null|\n",
      "|  10|20210219|   84181|  529807|                  173|        null|                  null|\n",
      "|  12|20210219| 1822644| 8956962|                 4298|       78590|                  null|\n",
      "|  13|20210219|  980411|    null|                 2973|       54434|                  null|\n",
      "+----+--------+--------+--------+---------------------+------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs['rearc_states_daily_test'].select('fips', 'date', 'positive', 'negative', 'hospitalizedcurrently', 'hospitalized', 'hospitalizeddischarged').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26418"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join select DataFrame fields together. enigma_jhu to rearc_states_daily_test\n",
    "dfs.keys()\n",
    "temp1 = dfs['enigma_jhu'] \\\n",
    "    .withColumn('fips', F.regexp_replace(F.col('fips'), r'^[0]*', '')) \\\n",
    "    .select('fips', 'province_state', 'country_region', 'confirmed', 'deaths', 'recovered', 'active')\n",
    "temp2 = dfs['rearc_states_daily_test'] \\\n",
    "    .select('fips', 'date', 'positive', 'negative', 'hospitalizedcurrently', 'hospitalized', 'hospitalizeddischarged')\n",
    "fact = temp1.join(temp2, temp1.fips == temp2.fips)\n",
    "fact.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[fips: string]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp1.select('fips').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| fips|\n",
      "+-----+\n",
      "| null|\n",
      "|10001|\n",
      "|10003|\n",
      "|10005|\n",
      "| 1001|\n",
      "| 1003|\n",
      "| 1005|\n",
      "| 1007|\n",
      "| 1009|\n",
      "| 1011|\n",
      "| 1013|\n",
      "| 1015|\n",
      "| 1017|\n",
      "| 1019|\n",
      "| 1021|\n",
      "| 1023|\n",
      "| 1025|\n",
      "| 1027|\n",
      "| 1029|\n",
      "| 1031|\n",
      "+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# temp1.select('fips').distinct().orderBy('fips').show()\n",
    "temp1.withColumn('fips', F.regexp_replace(F.col('fips'), r'^[0]*', '')).select('fips').distinct().orderBy('fips').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "awscovid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
